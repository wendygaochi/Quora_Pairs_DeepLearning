{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora NLP DeepLearning Project \n",
    "1. Prepare train and test data for network in local environment.\n",
    "2. Implement the network above and tune it in local environment with part of the train data. Parameters to be tuned are: number of neurons in each layer, learning rates (including recurrent_dropout of LSTM layer), standard deviation of *GaussianNoise* layer, *batch_size*. \n",
    "3. Run the model on the cluster with complete data and generate submission file as follows:\n",
    "\n",
    "*submission = pd.DataFrame({\"test_id\": test_id, \"is_duplicate\": prediction_prob})*  \n",
    "*submission.to_csv(\"submission1.csv\", index=False)*,\n",
    "\n",
    "where *prediction_prob* is 1D array of prediction probabilities, *test_id* is index from *test_id* column of *test.csv* file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of this project I will create the following set of features using PySpark:\n",
    "\n",
    "1. `lWCount1` - word count in lemma of question 1\n",
    "2. `lWCount2` - word count in lemma of question 2\n",
    "3. `qWCount1` - word count in question 1\n",
    "4. `qWCount2` - word count in question 2\n",
    "5. `lLen1` - length of lemma 1\n",
    "6. `lLen2` - length of lemma 2\n",
    "5. `qLen1` - length of question 1\n",
    "6. `qLen2` - length of question 2\n",
    "7. `lWCount_ratio` - ratio of lemmas word counts\n",
    "8. `qWCount_ratio` - ratio of questions word counts\n",
    "9. `lLen_ratio` - ratio of lemmas lengths\n",
    "10. `qLen_ratio` - ratio of questions lengths\n",
    "11. `lNgrams_1` - unigrams of lemmas \n",
    "12. `lNgrams_2` - bigrams of lemmas\n",
    "13. `lNgrams_3` - trigrams of lemmas\n",
    "14. `qNgrams_1` - unigrams of questions\n",
    "15. `qNgrams_2` - bigrams of questions\n",
    "16. `qNgrams_3` - trigrams of questions\n",
    "17. `qUnigram_ratio` - question unigram ratio\n",
    "18. `lUnigram_ratio` - lemma unigram ratio\n",
    "19. `tfidfDistance` - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.functions import isnan, when, count, length, lit, udf, col, struct\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.ml.feature import IDF, Tokenizer, CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/wendygao16/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/wendygao16/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"popular\")\n",
    "nltk.download(\"tagsets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFileName = \"/user/wendygao16/fs_Quora/train.csv\"\n",
    "testFileName = \"/user/wendygao16/fs_Quora/test.csv\"\n",
    "#outTrainFileName = \"./wendygao16/notebooks/Quora_NLP/train_features.csv\"\n",
    "#outTestFileName = \"./wendygao16/notebooks/Quora_NLP/test_features.csv\"\n",
    "#fitting_outPath = \"./wendygao16/notebooks/Quora_NLP/predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[test_id: string, question1: string, question2: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sch = StructType([StructField('id',IntegerType()), \\\n",
    "                  StructField('qid1',IntegerType()),\\\n",
    "                  StructField('qid2',IntegerType()), \\\n",
    "                  StructField('question1',StringType()),\\\n",
    "                  StructField('question2',StringType()), \\\n",
    "                  StructField('is_duplicate',IntegerType())])\n",
    "\n",
    "train = spark.read.csv(trainFileName, header=True, escape='\"', \n",
    "                       quote='\"',schema=sch, multiLine = True)\n",
    "train = train.dropna()\n",
    "train.cache()\n",
    "\n",
    "test = spark.read.csv(testFileName, header=True, escape='\"', \\\n",
    "                           encoding='utf8', multiLine = True)\n",
    "test = test.dropna()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------------------+--------------------+------------+\n",
      "| id|qid1|qid2|           question1|           question2|is_duplicate|\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "|  0|   1|   2|What is the step ...|What is the step ...|           0|\n",
      "|  1|   3|   4|What is the story...|What would happen...|           0|\n",
      "|  2|   5|   6|How can I increas...|How can Internet ...|           0|\n",
      "|  3|   7|   8|Why am I mentally...|Find the remainde...|           0|\n",
      "|  4|   9|  10|Which one dissolv...|Which fish would ...|           0|\n",
      "|  5|  11|  12|Astrology: I am a...|I'm a triple Capr...|           1|\n",
      "|  6|  13|  14| Should I buy tiago?|What keeps childe...|           0|\n",
      "|  7|  15|  16|How can I be a go...|What should I do ...|           1|\n",
      "|  8|  17|  18|When do you use ã‚·...|When do you use \"...|           0|\n",
      "|  9|  19|  20|Motorola (company...|How do I hack Mot...|           0|\n",
      "+---+----+----+--------------------+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|test_id|           question1|           question2|\n",
      "+-------+--------------------+--------------------+\n",
      "|      0|How does the Surf...|Why did Microsoft...|\n",
      "|      1|Should I have a h...|How much cost doe...|\n",
      "|      2|What but is the b...|What you send mon...|\n",
      "|      3|Which food not em...|   What foods fibre?|\n",
      "|      4|How \"aberystwyth\"...|How their can I s...|\n",
      "|      5|How are the two w...|I admire I am con...|\n",
      "|      6|How can I reduce ...|How can I reduce ...|\n",
      "|      7|By scrapping the ...|How will the rece...|\n",
      "|      8|What are the how ...|What are some of ...|\n",
      "|      9|After 12th years ...|Can a 14 old guy ...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop('qid1', 'qid2')\n",
    "maxTrainID = train.groupBy().max('id').collect()[0][0]\n",
    "test_ = test.withColumn(\"id\",(test.test_id+maxTrainID+1).cast(\"integer\")).drop('test_id')\n",
    "test_ = test.withColumn('is_duplicate', lit(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.union(test.select(train.columns))\n",
    "print('Number of rows = %s' % data.count())\n",
    "data.filter(data.id > -1).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function lemmas_nltk(s)\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "def lemmas_nltk(s):    \n",
    "    #remove out stop_words\n",
    "    words = [w for w in s.lower().split() if not w in stop_words]\n",
    "    \n",
    "    # Function *s.isalpha()* tests if *s* is non-empty and all characters in *s* are alphabetic.\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    # lemmatize to stem word\n",
    "    words = [wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(wordnet_lemmatizer.lemmatize(word,'a'),'v'),'n') for word in words]\n",
    "    return \" \".join(words)\n",
    "#Create lemmas_nltk_udf\n",
    "lemmas_nltk_udf = pyspark.sql.functions.udf(f=lemmas_nltk, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "ss=train.select('question2').take(2)[1][0] \n",
    "print(ss, lemmas_nltk(ss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordsCount_udf\n",
    "def wordsCount(s):\n",
    "    return len(nltk.word_tokenize(s))\n",
    "    #return len(s.split())\n",
    "wordsCount_udf = pyspark.sql.functions.udf(f=wordsCount, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_udf\n",
    "############## divide by zero occures!!! ###########################\n",
    "def ratio(x,y): return abs(x-y)/(x+y+1e-15)\n",
    "ratio_udf = udf(ratio, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonNgrams(s1,s2,n), commonNgrams_udf\n",
    "def commonNgrams(s1, s2, n):\n",
    "    regex = re.compile('([^\\s\\w|_])+', flags=0)\n",
    "\n",
    "    n = int(n)\n",
    "    ss1 = s1.lower().split()\n",
    "    ss2 = s2.lower().split()\n",
    "    ss1 = [regex.sub('', str) for str in ss1]\n",
    "    ss2 = [regex.sub('', str) for str in ss2]\n",
    "    \n",
    "    commonBigrams = (set(nltk.ngrams(ss1, n)) &set(nltk.ngrams(ss2, n))) \n",
    "    return len(commonBigrams)\n",
    "\n",
    "commonNgrams_udf = udf(commonNgrams, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram_ratio()\n",
    "def unigram_ratio(ngrams, n1, n2):\n",
    "    return ngrams/(1+max(n1, n2))\n",
    "\n",
    "unigram_ratio_udf = udf(unigram_ratio, DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureNames = ['lWCount1', 'lWCount2',\n",
    "                'qWCount1', 'qWCount2',\n",
    "                'lLen1', 'lLen2',\n",
    "                'qLen1', 'qLen2',\n",
    "                'lWCount_ratio', 'qWCount_ratio',\n",
    "                'lLen_ratio', 'qLen_ratio',\n",
    "                'qNgrams_1', 'qNgrams_2', 'qNgrams_3', \n",
    "                'lNgrams_1', 'lNgrams_2', 'lNgrams_3', \n",
    "                'qUnigram_ratio', 'lUnigram_ratio', #'qDouegram_ratio','lDouegram_ratio']\n",
    "                'tfidfDistance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions length, lemmas length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmas\n",
    "data = data.withColumn('lemma1', lemmas_nltk_udf(\"question1\"))\n",
    "data = data.withColumn('lemma2', lemmas_nltk_udf(\"question2\"))\n",
    "\n",
    "#data.select('id','lemma1','lemma2').show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: lWCount, qWCount, lLen, qLen\n",
    "for i in [\"1\",\"2\"]:\n",
    "    data = data.withColumn('lWCount'+i, wordsCount_udf(data['lemma'+i]))\n",
    "    data = data.withColumn('qWCount'+i, wordsCount_udf(data['question'+i]))\n",
    "    data = data.withColumn('lLen'+i, length(data['lemma'+i]))\n",
    "    data = data.withColumn('qLen'+i, length(data['question'+i]))\n",
    "\n",
    "#data.select('lWCount1','lWCount2','qWCount1','qWCount2','lLen1','lLen2','qLen1','qLen2').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lengths ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lengths ratios\n",
    "data = data.withColumn('lWCount_ratio', ratio_udf(\"lWCount1\", \"lWCount2\"))\n",
    "data = data.withColumn('qWCount_ratio', ratio_udf(\"qWCount1\",\"qWCount2\"))\n",
    "data = data.withColumn('lLen_ratio', ratio_udf(\"lLen1\", \"lLen2\"))\n",
    "data = data.withColumn('qLen_ratio', ratio_udf(\"qLen1\", \"qLen2\"))\n",
    "\n",
    "#data.select('lWCount_ratio','qWCount_ratio','lLen_ratio','qLen_ratio').show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N-grams and n-gram ratios\n",
    "#commonNgrams_udf\n",
    "for i in [\"1\",\"2\",\"3\"]:    \n",
    "    data = data.withColumn('lNgrams_'+i, commonNgrams_udf(\"lemma1\",\"lemma2\", lit(i)))\n",
    "    data = data.withColumn('qNgrams_'+i, commonNgrams_udf(\"question1\",\"question2\", lit(i)))\n",
    "    print(\"lNgrams_qNgrams_ done\")\n",
    "\n",
    "data = data.withColumn('lUnigram_ratio', unigram_ratio_udf( \"lNgrams_1\",\"lLen1\", \"lLen2\" ))\n",
    "print(\"lUnigram_ratio done\")\n",
    "\n",
    "data = data.withColumn('qUnigram_ratio', unigram_ratio_udf( \"qNgrams_1\",\"qLen1\", \"qLen2\" ))\n",
    "print(\"qUnigram_ratio done\")\n",
    "\n",
    "#data = data.withColumn('lDouegram_ratio', unigram_ratio_udf( \"lNgrams_2\",\"lLen1\", \"lLen2\" ))\n",
    "print(\"lDouegram_ratio done\")\n",
    "\n",
    "#data = data.withColumn('qDouegram_ratio', unigram_ratio_udf( \"qNgrams_2\",\"qLen1\", \"qLen2\" ))\n",
    "print(\"qDouegram_ratio done\")\n",
    "\n",
    "#data.select('lNgrams_1','lNgrams_2','lNgrams_3','qNgrams_1','qNgrams_2','qNgrams_3','qUnigram_ratio','lUnigram_ratio').show(6)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of lemmas\n",
    "tokenizer = Tokenizer(inputCol=\"lemma1\", outputCol=\"words1\") \n",
    "data = tokenizer.transform(data) \n",
    "tokenizer.setParams(inputCol=\"lemma2\", outputCol=\"words2\") \n",
    "data = tokenizer.transform(data) \n",
    "#data.select('id','lemma1','words1','lemma2','words2').show(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.selectExpr('words1 as words').join(data.selectExpr('words2 as words'), on='words', how='full')\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"tf\", minDF=2.0)\n",
    "cvModel = cv.fit(corpus)\n",
    "corpus = cvModel.transform(corpus)\n",
    "print('CountVectorizerModel has a vocabulary of length ',len(cvModel.vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = cvModel.transform(data.selectExpr('id', 'words1 as words'))\n",
    "res2 = cvModel.transform(data.selectExpr('id', 'words2 as words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Done\n"
     ]
    }
   ],
   "source": [
    "print(\"TF Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create IDF object, fit it to the whole corpus and calculate column \n",
    "#*\"idf\"* for both questions. Addcolumns *\"idf\"* to both *\"res1\"* and *\"res2\"*.\n",
    "idf = IDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "idfModel = idf.fit(corpus)\n",
    "res1 = idfModel.transform(res1)\n",
    "res2 = idfModel.transform(res2)\n",
    "res = res1.selectExpr('id','idf as idf1').join(res2.selectExpr('id','idf as idf2'), on='id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfDist(a,b): \n",
    "    return float(a.squared_distance(b))\n",
    "\n",
    "dist_udf = udf(tfidfDist, DoubleType())\n",
    "res = res.withColumn('dist', dist_udf(res['idf1'], res['idf2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF Done\n"
     ]
    }
   ],
   "source": [
    "print(\"IDF Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.join(res.selectExpr('id','dist as tfidfDistance'),on='id',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Join Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outData = data.select(['id']+featureNames+['is_duplicate'])\n",
    "print(\"outData ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outData.filter(outData.id <= maxTrainID).coalesce(1).write.csv(outTrainFileName,header=True,mode='overwrite',quote=\"\")\n",
    "print(\"output done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outData.filter(outData.id > maxTrainID).withColumn('id', outData.id-maxTrainID-1).coalesce(1).write.csv(outTestFileName,header=True,mode='overwrite',quote=\"\")\n",
    "print(\"output done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
